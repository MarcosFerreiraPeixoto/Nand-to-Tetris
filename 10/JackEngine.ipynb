{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re\n",
    "\n",
    "class JackTokenizer:\n",
    "    def __init__(self, file):\n",
    "        with open(file) as f:\n",
    "            '''Opens the input .jack file and gets ready to tokenize it.'''\n",
    "            self.code = f.readlines()\n",
    "            self.code = self.removeWhiteSpace(self.code)\n",
    "            \n",
    "            self.tokens = self.getTokens(self.code)\n",
    "            self.counter = 0\n",
    "            self.current_token = ''         \n",
    "\n",
    "    def hasMoreTokens (self):\n",
    "        '''Are there more commands in the input file?'''\n",
    "        \n",
    "        if self.counter < len(self.tokens):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def advance (self):\n",
    "        '''Reads the next token and makes it the current \n",
    "        token. Should be called only if hasMoreCommands()\n",
    "        is true. Initially there is no current command.'''\n",
    "\n",
    "        \n",
    "        self.current_token = self.tokens[self.counter]\n",
    "        self.counter += 1\n",
    "    \n",
    "    def tokenType (self):\n",
    "        '''Returns the type of the current token, as a\n",
    "        constant.'''\n",
    "        \n",
    "        key_words = ['class', 'constructor', 'function', 'method', 'field', 'static', 'var', 'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this', 'let', 'do', 'if', 'else', 'while', 'return']\n",
    "        symbols = ['{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&', '|', '<', '>', '=', '~', '&lt;', '&gt;', '&quot']\n",
    "\n",
    "        if self.current_token in key_words:\n",
    "            return 'keyword'\n",
    "        elif self.current_token in symbols:\n",
    "            return 'symbol'\n",
    "        elif self.current_token[0] == '\"' or self.current_token[0] == \"'\":\n",
    "            return 'string_const'\n",
    "        elif self.current_token.isnumeric():\n",
    "            return 'int_const'\n",
    "        else:\n",
    "            return 'identifier'\n",
    "\n",
    "    def keyWord (self):\n",
    "       return f'<keyword> {self.current_token} </keyword>\\n'\n",
    "    \n",
    "    def symbol (self):\n",
    "        symbol = self.current_token\n",
    "\n",
    "        if symbol == '<':\n",
    "            symbol = '&lt;'\n",
    "        elif symbol == '>':\n",
    "            symbol = '&gt;'\n",
    "        elif symbol == '&':\n",
    "            symbol = '&quot'\n",
    "\n",
    "        return f'<symbol> {symbol} </symbol>\\n'\n",
    "    \n",
    "    def identifier (self):\n",
    "        return f'<identifier> {self.current_token} </identifier>\\n'\n",
    "    \n",
    "    def intVal (self):\n",
    "        return f'<integerConstant> {self.current_token} </integerConstant>\\n'\n",
    "    \n",
    "    def stringVal (self):\n",
    "        return \"<stringConstant> \" + self.current_token.replace('\"','') + \"</stringConstant>\\n\"\n",
    "\n",
    "    @staticmethod\n",
    "    def removeWhiteSpace (code):\n",
    "        \n",
    "        code_without_white_space = []\n",
    "        \n",
    "        for line in code:\n",
    "            line = line.split('\\n', 1)[0]\n",
    "            line = line.split('//', 1)[0]\n",
    "            line = line.split('/**',1)[0]\n",
    "            line = line.split('/*',1)[0]\n",
    "            line = line.split('*',1)[0]\n",
    "            line = line.strip()\n",
    "            code_without_white_space.append(line)\n",
    "        \n",
    "        code_without_white_space = list(filter(None, code_without_white_space))\n",
    "        \n",
    "        return code_without_white_space\n",
    "\n",
    "    @staticmethod\n",
    "    def getTokens (code_without_white_space):\n",
    "        token_list = []\n",
    "\n",
    "        #Creating a list with all tokens\n",
    "        for code_line in code_without_white_space:\n",
    "            code_line = re.split('(\")', code_line) #Spliting the Strings\n",
    "            j = 0\n",
    "            while (j < len(code_line)):\n",
    "                \n",
    "                #Dealing with StringConstant Tokens\n",
    "                if code_line[j] == '\"':\n",
    "                    token_list.append('\"' + code_line[j+1] + '\"')\n",
    "                    j += 2\n",
    "                \n",
    "                #Dealing with all other tokens\n",
    "                else:\n",
    "                    tokens = re.split('(\\W)', code_line[j])\n",
    "                    for token in tokens:\n",
    "                        token_list.append(token)\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        token_list = [token for token in token_list if (token != '' and token != ' ')]\n",
    "\n",
    "        return token_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeWriter:\n",
    "    def __init__(self, file, tokenizer):\n",
    "        '''Opens the output file/stream and gets ready to write into it.'''\n",
    "        self.file_out = open(file, \"w\")\n",
    "        self.jmps = 0\n",
    "        #self.tokenizer = JackTokenizer('test.jack()') ##MODIFICAR PAR self.tokenizer = tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.type = ['int', 'char', 'boolean']\n",
    "        self.statements = ['let', 'if', 'while', 'do', 'return']\n",
    "        self.op = ['+', '-', '*', '/', '&', '|', '<', '>', '=']\n",
    "        \n",
    "    def compileClass (self):\n",
    "        '''Writes to the output file the assemply code that implements the\n",
    "        givem arithmetic command.'''\n",
    "\n",
    "        # 'class'\n",
    "        self.file_out.write('<class>\\n')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "        \n",
    "        # 'className'\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # '{'\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # 'classVarDec'*\n",
    "        if self.tokenizer.current_token in ['static', 'field']:\n",
    "            while (self.tokenizer.current_token in ['static', 'field']):\n",
    "                self.compileClassVarDec()\n",
    "                self.tokenizer.advance()\n",
    "          \n",
    "        # 'subroutineDec'*\n",
    "        if self.tokenizer.current_token in ['constructor', 'function', 'method']:          \n",
    "            while (self.tokenizer.current_token in ['constructor', 'function', 'method']):\n",
    "                self.compileSubroutine()\n",
    "                self.tokenizer.advance()\n",
    "      \n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</class>\\n')\n",
    "\n",
    "\n",
    "    def compileClassVarDec (self):\n",
    "        \n",
    "        self.file_out.write('<classVarDec>\\n')\n",
    "\n",
    "        # ('static' | 'field')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type | className\n",
    "        if (self.tokenizer.current_token in self.type) or (self.tokenizer.tokenType() == 'identifier'):\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token == ','):\n",
    "            \n",
    "            self.compile()\n",
    "            self.tokenizer.advance()           \n",
    "            \n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</classVarDec>\\n')\n",
    "    \n",
    "    def varDec(self):\n",
    "        \n",
    "        self.file_out.write('<varDec>\\n')\n",
    "\n",
    "        # ('var')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type\n",
    "        if (self.tokenizer.current_token in self.type) or self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token != ';'):\n",
    "            \n",
    "            # ','\n",
    "            if self.tokenizer.current_token == ',':\n",
    "                self.compile()     \n",
    "                self.tokenizer.advance()         \n",
    "            else:\n",
    "                raise Exception(\"Expected ',' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</varDec>\\n')\n",
    "\n",
    "    def compile (self):\n",
    "        token_type = self.tokenizer.tokenType()\n",
    "\n",
    "        if token_type == 'keyword': \n",
    "            self.file_out.write(self.tokenizer.keyWord())\n",
    "\n",
    "        elif token_type == 'symbol':\n",
    "            self.file_out.write(self.tokenizer.symbol())\n",
    "\n",
    "        elif token_type == 'int_const':\n",
    "            self.file_out.write(self.tokenizer.intVal())\n",
    "\n",
    "        elif token_type == 'string_const':\n",
    "            self.file_out.write(self.tokenizer.stringVal())\n",
    "        \n",
    "        elif token_type == 'identifier':\n",
    "            self.file_out.write(self.tokenizer.identifier())\n",
    "        \n",
    "        print(self.tokenizer.current_token)\n",
    "\n",
    "    def compileTerm (self):\n",
    "        self.file_out.write(f\"<term>\\n\")\n",
    "        \n",
    "        # integerConstant | stringConstant | keywordConstant | varName | varName'[' expression ']' \n",
    "        # | '(' expression ')' | (unaryOp term) | subroutineCall\n",
    "        token_type = self.tokenizer.tokenType()\n",
    "\n",
    "        # integerConstant\n",
    "        if token_type == 'int_const':\n",
    "            self.compile()\n",
    "\n",
    "        # stringConstant\n",
    "        elif token_type == 'string_const':\n",
    "            self.compile()\n",
    "            \n",
    "        # keywordConstant\n",
    "        elif self.tokenizer.current_token in ['true', 'false', 'null', 'this']:\n",
    "            #Não sei se é só <keyword>\\n ou <keywordConstant>\\n\n",
    "            self.compile()\n",
    "            #self.file_out.write(f\"<keywordConstant>\\n {self.tokenizer.current_token} </keywordConstant>\\n\")\n",
    "\n",
    "        # varName | varName'[' expression ']' | subroutineName '(' expressionList ')' | \n",
    "        # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "        elif token_type == 'identifier':\n",
    "                \n",
    "            #varName \n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "                \n",
    "            # varName '[' expression ']' \n",
    "            if self.tokenizer.current_token == '[':\n",
    "                # '['\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                    \n",
    "                # expression\n",
    "                self.compileExpression()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # ']'\n",
    "                if self.tokenizer.current_token == ']':\n",
    "                    self.compile()\n",
    "                else:\n",
    "                    raise Exception(\"Expected ']' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "            #subroutineName '(' expressionList ')' \n",
    "            elif self.tokenizer.current_token == '(':\n",
    "                # '('\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # expressionList\n",
    "                self.compileExpressionList()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # ')'\n",
    "                if self.tokenizer.current_token == ')':\n",
    "                    self.compile()\n",
    "                else:\n",
    "                   raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "            # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "            elif self.tokenizer.current_token == '.':\n",
    "                # '.'\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                    \n",
    "                # subroutineName\n",
    "                if self.tokenizer.tokenType() == 'identifier':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                   raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "                # '('\n",
    "                if self.tokenizer.current_token == '(':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                   raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "                # expressionList\n",
    "                self.compileExpressionList()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # ')'\n",
    "                if self.tokenizer.current_token == ')':\n",
    "                    self.compile()\n",
    "                else:\n",
    "                   raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # varName\n",
    "            else:\n",
    "                self.tokenizer.counter -= 1 #Retrocendo 1\n",
    "            \n",
    "        # '(' expression ')'\n",
    "        elif self.tokenizer.current_token == '(':\n",
    "               self.compile()\n",
    "               self.tokenizer.advance()\n",
    "\n",
    "               self.compileExpression()\n",
    "               self.tokenizer.advance()\n",
    "\n",
    "               if self.tokenizer.current_token == ')':\n",
    "                   self.compile()\n",
    "               else:\n",
    "                   raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "        # (unaryOp term)\n",
    "        elif self.tokenizer.current_token in ['-', '~']:\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            self.compileTerm()\n",
    "            \n",
    "        self.file_out.write(f\"</term>\\n\")\n",
    "\n",
    "    def compileSubroutine (self):\n",
    "\n",
    "        self.file_out.write('<subroutineDec>\\n')\n",
    "\n",
    "        #('constructor' | 'function' | 'method')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #('void' | 'type' | className)\n",
    "        if self.tokenizer.current_token == 'void':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        elif (self.tokenizer.current_token in self.type) or (self.tokenizer.tokenType() == 'identifier'):\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        #subroutineName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        #'('\n",
    "        if self.tokenizer.current_token == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '('\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # parameterList\n",
    "        self.compileParameterList() #Does not need tokenizer.advance() after.\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.current_token == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # subroutineBody\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compileSubroutineBody()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</subroutineDec>\\n')\n",
    "\n",
    "        \n",
    "    def compileSubroutineBody(self):\n",
    "        \n",
    "        self.file_out.write('<subroutineBody>\\n')\n",
    "\n",
    "        # '{'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # varDec*\n",
    "        while self.tokenizer.current_token == 'var':\n",
    "            self.varDec()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</subroutineBody>\\n')\n",
    "\n",
    "    def compileStatements(self):\n",
    "        self.file_out.write('<statements>\\n')\n",
    "\n",
    "        while(self.tokenizer.current_token in self.statements):\n",
    "            self.compileStatement()\n",
    "            self.tokenizer.advance()\n",
    "        \n",
    "        self.tokenizer.counter -= 1\n",
    "\n",
    "        self.file_out.write('</statements>\\n')\n",
    "\n",
    "    def compileStatement(self):\n",
    "\n",
    "        if self.tokenizer.current_token == 'let':\n",
    "            self.compileLet()\n",
    "        elif self.tokenizer.current_token == 'if':\n",
    "            self.compileIf()\n",
    "        elif self.tokenizer.current_token == 'while':\n",
    "            self.compileWhile()\n",
    "        elif self.tokenizer.current_token == 'do':\n",
    "            self.compileDo()\n",
    "        elif self.tokenizer.current_token == 'return':\n",
    "            self.compileReturn()\n",
    "\n",
    "    def compileLet(self):\n",
    "        # 'let' varName('[' expression ']')? '=' expression ';'\n",
    "        self.file_out.write('<letStatement>\\n')\n",
    "        \n",
    "        # 'let'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        # ('[' expression ']')?\n",
    "        if self.tokenizer.current_token == '[':\n",
    "            # '['\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "            \n",
    "            # expression\n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # ']'\n",
    "            if self.tokenizer.current_token == ']':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected ']' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        if self.tokenizer.current_token == '=':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '=' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # ';'\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</letStatement>\\n')\n",
    "    \n",
    "    def compileIf(self):\n",
    "        # 'if' '(' expression ')' '{' statements '}' ('else' '{' statements '}')?\n",
    "        self.file_out.write('<ifStatement>\\n')\n",
    "        # 'if' '(' expression ')' '{' statements '}'\n",
    "        # 'if'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '('\n",
    "        if self.tokenizer.current_token == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.current_token == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # '{'\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "\n",
    "        # ('else' '{' statements '}')?\n",
    "        if self.tokenizer.current_token == 'else':\n",
    "            #else\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # '{'\n",
    "            if self.tokenizer.current_token == '{':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # statements\n",
    "            self.compileStatements()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # '}'\n",
    "            if self.tokenizer.current_token == '}':\n",
    "                self.compile()\n",
    "            else:\n",
    "                raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        #If there is no else statement\n",
    "        else:\n",
    "            self.tokenizer.counter -= 1\n",
    "\n",
    "        self.file_out.write('</ifStatement>\\n')\n",
    "    \n",
    "    def compileWhile(self):\n",
    "        # 'while' '(' expression ')' '{' statements '}'\n",
    "        self.file_out.write('<whileStatement>\\n')\n",
    "\n",
    "        # 'while'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '('\n",
    "        if self.tokenizer.current_token == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.current_token == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # '{'\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</whileStatement>\\n')\n",
    "    \n",
    "    def compileDo(self):\n",
    "        # 'do' subroutineCall ';'\n",
    "        self.file_out.write('<doStatement>\\n')\n",
    "\n",
    "        # 'do'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # subroutineCall\n",
    "        # subroutineName '(' expressionList ')' | \n",
    "        # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                \n",
    "            # subroutineName | (className | varName)\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "                \n",
    "                    \n",
    "            #subroutineName '(' expressionList ')' \n",
    "            if self.tokenizer.current_token == '(':\n",
    "                # '('\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # expressionList\n",
    "                self.compileExpressionList()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # ')'\n",
    "                if self.tokenizer.current_token == ')':\n",
    "                    self.compile()\n",
    "                else:\n",
    "                   raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "            # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "            elif self.tokenizer.current_token == '.':\n",
    "                # '.'\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                    \n",
    "                # subroutineName\n",
    "                if self.tokenizer.tokenType() == 'identifier':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                   raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "                # '('\n",
    "                if self.tokenizer.current_token == '(':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                   raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "                # expressionList\n",
    "                self.compileExpressionList()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # ')'\n",
    "                if self.tokenizer.current_token == ')':\n",
    "                    self.compile()\n",
    "                else:\n",
    "                   raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # ';'\n",
    "        if self.tokenizer.current_token == ';':\n",
    "            self.compile()\n",
    "    \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</doStatement>\\n')\n",
    "\n",
    "    def compileReturn(self):\n",
    "        self.file_out.write('<returnStatement>\\n')\n",
    "\n",
    "        # 'return'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # expression\n",
    "        if self.tokenizer.current_token != ';':\n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "        \n",
    "        # ';'\n",
    "        if self.tokenizer.current_token == ';':\n",
    "            self.compile()\n",
    "           \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")    \n",
    "\n",
    "        self.file_out.write('</returnStatement>\\n')\n",
    "\n",
    "    def compileExpressionList(self):\n",
    "        #(expression(',' expression)*)?\n",
    "        self.file_out.write('<expressionList>\\n')\n",
    "\n",
    "        #(expression(',' expression)*)?\n",
    "        if self.tokenizer.tokenType() in ['int_const', 'string_const', 'identifier'] or self.tokenizer.current_token in ['true', 'false', 'null', 'this']:\n",
    "            # expression\n",
    "            \n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # (',' expression)*\n",
    "            while self.tokenizer.current_token == ',':\n",
    "                # ','\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # expression\n",
    "                self.compileExpression()\n",
    "                self.tokenizer.advance()\n",
    "            \n",
    "            #Atrasando o contador em\n",
    "        \n",
    "        self.tokenizer.counter -= 1\n",
    "\n",
    "        self.file_out.write('</expressionList>\\n')\n",
    "\n",
    "    def compileExpression(self):\n",
    "        # term (op term)*\n",
    "        self.file_out.write('<expression>\\n')\n",
    "\n",
    "        # term\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # (op term)*\n",
    "        while self.tokenizer.current_token in self.op:\n",
    "            # op\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # term\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        #Andando com o contador para trás\n",
    "        self.tokenizer.counter -= 1\n",
    "        \n",
    "        self.file_out.write('</expression>\\n')\n",
    "\n",
    "    def compileParameterList(self):\n",
    "        self.file_out.write('<parameterList>\\n')\n",
    "        \n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            # type\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            #(',' type varName)*\n",
    "            while (self.tokenizer.current_token == ','):\n",
    "                # ','\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                \n",
    "                # type\n",
    "                if self.tokenizer.current_token in self.type:\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else: \n",
    "                    raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "                # varName\n",
    "                if self.tokenizer.tokenType() == 'identifier':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                    raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</parameterList>\\n')\n",
    "\n",
    "    def Close (self):\n",
    "        '''Closes the output file.'''\n",
    "\n",
    "        self.file_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'SquareGame', '{', 'field', 'Square', 'square', ';', 'field', 'int', 'direction', ';', 'constructor', 'SquareGame', 'new', '(', ')', '{', 'let', 'square', '=', 'Square', '.', 'new', '(', '0', ',', '0', ',', '30', ')', ';', 'let', 'direction', '=', '0', ';', 'return', 'this', ';', '}', 'method', 'void', 'dispose', '(', ')', '{', 'do', 'square', '.', 'dispose', '(', ')', ';', 'do', 'Memory', '.', 'deAlloc', '(', 'this', ')', ';', 'return', ';', '}', 'method', 'void', 'moveSquare', '(', ')', '{', 'if', '(', 'direction', '=', '1', ')', '{', 'do', 'square', '.', 'moveUp', '(', ')', ';', '}', 'if', '(', 'direction', '=', '2', ')', '{', 'do', 'square', '.', 'moveDown', '(', ')', ';', '}', 'if', '(', 'direction', '=', '3', ')', '{', 'do', 'square', '.', 'moveLeft', '(', ')', ';', '}', 'if', '(', 'direction', '=', '4', ')', '{', 'do', 'square', '.', 'moveRight', '(', ')', ';', '}', 'do', 'Sys', '.', 'wait', '(', '5', ')', ';', 'return', ';', '}', 'method', 'void', 'run', '(', ')', '{', 'var', 'char', 'key', ';', 'var', 'boolean', 'exit', ';', 'let', 'exit', '=', 'false', ';', 'while', '(', '~', 'exit', ')', '{', 'while', '(', 'key', '=', '0', ')', '{', 'let', 'key', '=', 'Keyboard', '.', 'keyPressed', '(', ')', ';', 'do', 'moveSquare', '(', ')', ';', '}', 'if', '(', 'key', '=', '81', ')', '{', 'let', 'exit', '=', 'true', ';', '}', 'if', '(', 'key', '=', '90', ')', '{', 'do', 'square', '.', 'decSize', '(', ')', ';', '}', 'if', '(', 'key', '=', '88', ')', '{', 'do', 'square', '.', 'incSize', '(', ')', ';', '}', 'if', '(', 'key', '=', '131', ')', '{', 'let', 'direction', '=', '1', ';', '}', 'if', '(', 'key', '=', '133', ')', '{', 'let', 'direction', '=', '2', ';', '}', 'if', '(', 'key', '=', '130', ')', '{', 'let', 'direction', '=', '3', ';', '}', 'if', '(', 'key', '=', '132', ')', '{', 'let', 'direction', '=', '4', ';', '}', 'while', '(', '~', '(', 'key', '=', '0', ')', ')', '{', 'let', 'key', '=', 'Keyboard', '.', 'keyPressed', '(', ')', ';', 'do', 'moveSquare', '(', ')', ';', '}', '}', 'return', ';', '}', '}']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = JackTokenizer('SquareGame.jack')\n",
    "print(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n"
     ]
    }
   ],
   "source": [
    "tokenizer.advance()\n",
    "print(tokenizer.current_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = CodeWriter('teste2.xml', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "SquareGame\n",
      "{\n",
      "field\n",
      "Square\n",
      "square\n",
      ";\n",
      "field\n",
      "int\n",
      "direction\n",
      ";\n",
      "constructor\n",
      "SquareGame\n",
      "new\n",
      "(\n",
      ")\n",
      "{\n",
      "let\n",
      "square\n",
      "=\n",
      "Square\n",
      ".\n",
      "new\n",
      "(\n",
      "0\n",
      ",\n",
      "0\n",
      ",\n",
      "30\n",
      ")\n",
      ";\n",
      "let\n",
      "direction\n",
      "=\n",
      "0\n",
      ";\n",
      "return\n",
      "this\n",
      ";\n",
      "}\n",
      "method\n",
      "void\n",
      "dispose\n",
      "(\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "dispose\n",
      "(\n",
      ")\n",
      ";\n",
      "do\n",
      "Memory\n",
      ".\n",
      "deAlloc\n",
      "(\n",
      "this\n",
      ")\n",
      ";\n",
      "return\n",
      ";\n",
      "}\n",
      "method\n",
      "void\n",
      "moveSquare\n",
      "(\n",
      ")\n",
      "{\n",
      "if\n",
      "(\n",
      "direction\n",
      "=\n",
      "1\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "moveUp\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "direction\n",
      "=\n",
      "2\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "moveDown\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "direction\n",
      "=\n",
      "3\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "moveLeft\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "direction\n",
      "=\n",
      "4\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "moveRight\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "do\n",
      "Sys\n",
      ".\n",
      "wait\n",
      "(\n",
      "5\n",
      ")\n",
      ";\n",
      "return\n",
      ";\n",
      "}\n",
      "method\n",
      "void\n",
      "run\n",
      "(\n",
      ")\n",
      "{\n",
      "var\n",
      "char\n",
      "key\n",
      ";\n",
      "var\n",
      "boolean\n",
      "exit\n",
      ";\n",
      "let\n",
      "exit\n",
      "=\n",
      "false\n",
      ";\n",
      "while\n",
      "(\n",
      "~\n",
      "exit\n",
      ")\n",
      "{\n",
      "while\n",
      "(\n",
      "key\n",
      "=\n",
      "0\n",
      ")\n",
      "{\n",
      "let\n",
      "key\n",
      "=\n",
      "Keyboard\n",
      ".\n",
      "keyPressed\n",
      "(\n",
      ")\n",
      ";\n",
      "do\n",
      "moveSquare\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "81\n",
      ")\n",
      "{\n",
      "let\n",
      "exit\n",
      "=\n",
      "true\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "90\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "decSize\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "88\n",
      ")\n",
      "{\n",
      "do\n",
      "square\n",
      ".\n",
      "incSize\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "131\n",
      ")\n",
      "{\n",
      "let\n",
      "direction\n",
      "=\n",
      "1\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "133\n",
      ")\n",
      "{\n",
      "let\n",
      "direction\n",
      "=\n",
      "2\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "130\n",
      ")\n",
      "{\n",
      "let\n",
      "direction\n",
      "=\n",
      "3\n",
      ";\n",
      "}\n",
      "if\n",
      "(\n",
      "key\n",
      "=\n",
      "132\n",
      ")\n",
      "{\n",
      "let\n",
      "direction\n",
      "=\n",
      "4\n",
      ";\n",
      "}\n",
      "while\n",
      "(\n",
      "~\n",
      "(\n",
      "key\n",
      "=\n",
      "0\n",
      ")\n",
      ")\n",
      "{\n",
      "let\n",
      "key\n",
      "=\n",
      "Keyboard\n",
      ".\n",
      "keyPressed\n",
      "(\n",
      ")\n",
      ";\n",
      "do\n",
      "moveSquare\n",
      "(\n",
      ")\n",
      ";\n",
      "}\n",
      "}\n",
      "return\n",
      ";\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "engine.compileClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    file_dir = input('Please, insert the file name:')\n",
    "\n",
    "    if file_dir[-1] == '/': #Verifying if the input is a file or a folder\n",
    "        os.chdir(file_dir)\n",
    "        files = glob.glob(\"*.jack\")\n",
    "        dir_name = file_dir.split('/')[-2]\n",
    "        \n",
    "        #code_writer = CodeWriter(dir_name + '.asm')\n",
    "        \n",
    "        # if 'Sys.jack' in files:\n",
    "        #     code_writer.writeCall (arg1='Sys.init', arg2='0')\n",
    "\n",
    "    else:\n",
    "        files = [file_dir]\n",
    "        #code_writer = CodeWriter(files[0] + '.asm')\n",
    "\n",
    "    \n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        arg1 = ''\n",
    "        arg2 = ''\n",
    "\n",
    "        tokenizer = JackTokenizer(file)\n",
    "\n",
    "        while (tokenizer.hasMoreTokens() == True):\n",
    "            tokenizer.advance()\n",
    "            token_type = tokenizer.tokenType()\n",
    "            \n",
    "            if token_type == 'keyword':  \n",
    "                key_word = tokenizer.keyWord()\n",
    "                #code_writer.writePushPop(token_type, arg1, arg2, file)\n",
    "\n",
    "            elif token_type == 'symbol':\n",
    "                symbol = tokenizer.symbol()\n",
    "                #code_writer.writeArithmetic(arg1)\n",
    "\n",
    "            elif token_type == 'identifier':\n",
    "                identifier = tokenizer.identifier()\n",
    "                #code_writer.writeLabel(arg1)\n",
    "\n",
    "            elif token_type == 'int_val':\n",
    "                int_val = tokenizer.intVal()\n",
    "                #code_writer.writeGoto(arg1)\n",
    "\n",
    "            elif token_type == 'string_val':\n",
    "                string_val = tokenizer.stringVal()\n",
    "                #code_writer.writeIf(arg1)\n",
    "\n",
    "    #code_writer.Close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adc1fd1a4b8c764c52852e42b10ea6aaf40f3ebe47b5be7cd49ce1732141673f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
