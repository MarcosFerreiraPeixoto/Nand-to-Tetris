{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re\n",
    "\n",
    "class JackTokenizer:\n",
    "    def __init__(self, file):\n",
    "        with open(file) as f:\n",
    "            '''Opens the input .jack file and gets ready to tokenize it.'''\n",
    "            self.code = f.readlines()\n",
    "            self.code = self.removeWhiteSpace(self.code)\n",
    "            \n",
    "            self.tokens = self.getTokens(self.code)\n",
    "            self.counter = 0\n",
    "            self.current_token = ''         \n",
    "\n",
    "    def hasMoreTokens (self):\n",
    "        '''Are there more commands in the input file?'''\n",
    "        \n",
    "        if self.counter < len(self.tokens):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def advance (self):\n",
    "        '''Reads the next token and makes it the current \n",
    "        token. Should be called only if hasMoreCommands()\n",
    "        is true. Initially there is no current command.'''\n",
    "\n",
    "        \n",
    "        self.current_token = self.tokens[self.counter]\n",
    "        self.counter += 1\n",
    "    \n",
    "    def tokenType (self):\n",
    "        '''Returns the type of the current token, as a\n",
    "        constant.'''\n",
    "        \n",
    "        key_words = ['class', 'constructor', 'function', 'method', 'field', 'static', 'var', 'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this', 'let', 'do', 'if', 'else', 'while', 'return']\n",
    "        symbols = ['{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&', '|', '<', '>', '=', '~']\n",
    "   \n",
    "        if self.current_token in key_words:\n",
    "            return 'keyword'\n",
    "        elif self.current_token in symbols:\n",
    "            return 'symbol'\n",
    "        elif self.current_token.isnumeric():\n",
    "            return 'int_const'\n",
    "        elif self.current_token[0] == '\"':\n",
    "            return 'string_const'\n",
    "        else:\n",
    "            return 'identifier'\n",
    "\n",
    "    def keyWord (self):\n",
    "       return f'<keyword> {self.current_token} </keyword>'\n",
    "    \n",
    "    def symbol (self):\n",
    "        return f'<symbol> {self.current_token} </symbol>'\n",
    "    \n",
    "    def identifier (self):\n",
    "        return f'<identifier> {self.current_token} </identifier>'\n",
    "    \n",
    "    def intVal (self):\n",
    "        return f'<integerConstant> {self.current_token} </integerConstant>'\n",
    "    \n",
    "    def stringVal (self):\n",
    "        return \"<stringConstant>\" + self.current_token.replace('\"','') + \"</stringConstant>\"\n",
    "\n",
    "    @staticmethod\n",
    "    def removeWhiteSpace (code):\n",
    "        \n",
    "        code_without_white_space = []\n",
    "        \n",
    "        for line in code:\n",
    "            line = line.split('\\n', 1)[0]\n",
    "            line = line.split('//', 1)[0]\n",
    "            line = line.split('/**',1)[0]\n",
    "            line = line.split('/*',1)[0]\n",
    "            line = line.strip()\n",
    "            code_without_white_space.append(line)\n",
    "        \n",
    "        code_without_white_space = list(filter(None, code_without_white_space))\n",
    "        \n",
    "        return code_without_white_space\n",
    "\n",
    "    @staticmethod\n",
    "    def getTokens (code_without_white_space):\n",
    "        token_list = []\n",
    "\n",
    "        #Creating a list with all tokens\n",
    "        for code_line in code_without_white_space:\n",
    "            code_line = re.split('(\")', code_line) #Spliting the Strings\n",
    "            j = 0\n",
    "            while (j < len(code_line)):\n",
    "                \n",
    "                #Dealing with StringConstant Tokens\n",
    "                if code_line[j] == '\"':\n",
    "                    token_list.append('\"' + code_line[j+1] + '\"')\n",
    "                    j += 2\n",
    "                \n",
    "                #Dealing with all other tokens\n",
    "                else:\n",
    "                    tokens = re.split('(\\W)', code_line[j])\n",
    "                    for token in tokens:\n",
    "                        token_list.append(token)\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        token_list = [token for token in token_list if (token != '' and token != ' ')]\n",
    "\n",
    "        return token_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeWriter:\n",
    "    def __init__(self, file, tokenizer):\n",
    "        '''Opens the output file/stream and gets ready to write into it.'''\n",
    "        self.file_out = open(file, \"w\")\n",
    "        self.jmps = 0\n",
    "        self.tokenizer = JackTokenizer('test.jack()') ##MODIFICAR PAR self.tokenizer = tokenizer\n",
    "\n",
    "        self.type = ['int', 'char', 'boolean']\n",
    "        self.statements = ['let', 'if', 'while', 'do', 'return']\n",
    "        self.op = ['+', '-', '*', '/', '&', '|', '<', '>', '=']\n",
    "\n",
    "        self.file_out.write('<tokens>')\n",
    "        \n",
    "    def compileClass (self):\n",
    "        '''Writes to the output file the assemply code that implements the\n",
    "        givem arithmetic command.'''\n",
    "\n",
    "        # 'class'\n",
    "        self.file_out.write('<class>')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "        \n",
    "        # 'className'\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # '{'\n",
    "        if self.tokenizer.symbol() == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{'\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # 'classVarDec'*\n",
    "        if self.tokenizer.current_token in ['static', 'field']:\n",
    "            while (self.tokenizer.current_token in ['static', 'field']):\n",
    "                self.compileClassVarDec()\n",
    "                self.tokenizer.advance()\n",
    "          \n",
    "        # 'subroutineDec'*\n",
    "        if self.tokenizer.current_token in ['constructor', 'function', 'method']:          \n",
    "            while (self.tokenizer.current_token in ['constructor', 'function', 'method']):\n",
    "                self.compileSubroutine()\n",
    "                self.tokenizer.advance()\n",
    "      \n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</class>')\n",
    "\n",
    "\n",
    "    def compileClassVarDec (self):\n",
    "        \n",
    "        self.file_out.write('<classVarDec>')\n",
    "\n",
    "        # ('static' | 'field')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type\n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token == ','):\n",
    "            \n",
    "            self.compile()\n",
    "            self.tokenizer.advance()           \n",
    "            \n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</classVarDec>')\n",
    "    \n",
    "    def VarDec (self):\n",
    "        \n",
    "        self.file_out.write('<VarDec>')\n",
    "\n",
    "        # ('var')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type\n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token != ';'):\n",
    "            \n",
    "            # ','\n",
    "            if self.tokenizer.current_token == ',':\n",
    "                self.compile()              \n",
    "            else:\n",
    "                raise Exception(\"Expected ',' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</VarDec>')\n",
    "\n",
    "    def compile (self):\n",
    "        token_type = self.tokenizer.tokenType()\n",
    "\n",
    "        if token_type == 'keyword': \n",
    "            self.file_out.write(self.tokenizer.keyWord())\n",
    "\n",
    "        elif token_type == 'symbol':\n",
    "            self.file_out.write(self.tokenizer.symbol())\n",
    "\n",
    "        elif token_type == 'int_val':\n",
    "            self.file_out.write(self.tokenizer.intVal())\n",
    "\n",
    "        elif token_type == 'string_val':\n",
    "            self.file_out.write(self.tokenizer.stringVal())\n",
    "        \n",
    "        elif token_type == 'identifier':\n",
    "            self.file_out.write(self.tokenizer.identifier())\n",
    "\n",
    "    def compileTerm (self):\n",
    "            self.file_out.write(f\"<term>\")\n",
    "            \n",
    "            # integerConstant | stringConstant | keywordConstant | varName | varName'[' expression ']' \n",
    "            # | '(' expression ')' | (unaryOp term) | subroutineCall\n",
    "            token_type = self.tokenizer.tokenType()\n",
    "\n",
    "            # integerConstant\n",
    "            if token_type == 'int_val':\n",
    "                self.compile()\n",
    "\n",
    "            # stringConstant\n",
    "            elif token_type == 'string_val':\n",
    "                self.compile()\n",
    "            \n",
    "            # keywordConstant\n",
    "            elif self.tokenizer.current_token in ['true', 'false', 'null', 'this']:\n",
    "                #Não sei se é só <keyword> ou <keywordConstant>\n",
    "                self.compile()\n",
    "                #self.file_out.write(f\"<keywordConstant> {self.tokenizer.current_token} </keywordConstant>\")\n",
    "\n",
    "            # varName | varName'[' expression ']' | subroutineName '(' expressionList ')' | \n",
    "            # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "            elif token_type == 'identifier':\n",
    "                \n",
    "                #varName \n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                \n",
    "                # varName '[' expression ']' \n",
    "                if self.tokenizer.current_token == '[':\n",
    "                    # '['\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                    \n",
    "                    # expression\n",
    "                    self.compileExpression()\n",
    "                    self.tokenizer.advance()\n",
    "\n",
    "                    # ']'\n",
    "                    if self.tokenizer.current_token == ']':\n",
    "                        self.compile()\n",
    "                    else:\n",
    "                        raise Exception(\"Expected ']' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "                #subroutineName '(' expressionList ')' \n",
    "                elif self.tokenizer.current_token == '(':\n",
    "                    # '('\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "\n",
    "                    # expressionList\n",
    "                    self.compileExpressionList()\n",
    "                    self.tokenizer.advance()\n",
    "\n",
    "                    # ')'\n",
    "                    if self.tokenizer.current_token == ')':\n",
    "                        self.compile()\n",
    "                    else:\n",
    "                       raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "                # (className | varName)'.'subroutineName'(' expressionList ')'\n",
    "                elif self.tokenizer.current_token == '.':\n",
    "                    # '.'\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                    \n",
    "                    # subroutineName\n",
    "                    if self.tokenizer.tokenType == 'identifier':\n",
    "                        self.compile()\n",
    "                        self.tokenizer.advance()\n",
    "                    else:\n",
    "                       raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "                \n",
    "                    # '('\n",
    "                    if self.tokenizer.current_token == '(':\n",
    "                        self.compile()\n",
    "                        self.tokenizer.advance()\n",
    "                    else:\n",
    "                       raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "                    # expressionList\n",
    "                    self.compileExpressionList()\n",
    "                    self.advance()\n",
    "\n",
    "                    # ')'\n",
    "                    if self.tokenizer.current_token == ')':\n",
    "                        self.compile()\n",
    "                    else:\n",
    "                       raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "                # varName\n",
    "                else:\n",
    "                    self.tokenizer.counter -= 1 #Retrocendo 1\n",
    "            \n",
    "            # '(' expression ')'\n",
    "            elif self.tokenizer.current_token == '(':\n",
    "                   self.compile()\n",
    "                   self.tokenizer.advance()\n",
    "\n",
    "                   self.compileExpression()\n",
    "                   self.tokenizer.advance()\n",
    "\n",
    "                   if self.tokenizer.current_token == ')':\n",
    "                       self.compile()\n",
    "                   else:\n",
    "                       raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "            # (unaryOp term)\n",
    "            elif self.tokenizer.current_token in ['-', '~']:\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                self.compileTerm()\n",
    "            \n",
    "            self.file_out.write(f\"</term>\")\n",
    "\n",
    "    def compileSubroutine (self):\n",
    "\n",
    "        self.file_out.write('<subroutineDec>')\n",
    "\n",
    "        #('constructor' | 'function' | 'method')\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #('void' | 'type')\n",
    "        if self.tokenizer.current_token == 'void':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        elif self.tokenizer.current_token in self.type:\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        #subroutineName\n",
    "        if self.tokenizer.tokenType == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected identifier\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        #'('\n",
    "        if self.tokenizer.symbol() == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '('\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # parameterList\n",
    "        self.compileParameterList() #Does not need tokenizer.advance() after.\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.symbol() == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # subroutineBody\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compileSubroutineBody()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</subroutineDec>')\n",
    "\n",
    "        \n",
    "    def compileSubroutineBody(self):\n",
    "        \n",
    "        self.file_out.write('<subroutineBody>')\n",
    "\n",
    "        # '{'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # varDec*\n",
    "        while self.tokenizer.current_token == 'var':\n",
    "            self.varDec()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compile()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</subroutineBody>')\n",
    "\n",
    "    def compileStatements(self):\n",
    "        self.file_out.write('<statements>')\n",
    "\n",
    "        while(self.tokenizer.current_token in self.statements):\n",
    "            self.compileStatement()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        self.file_out.write('</statements>')\n",
    "\n",
    "    def compileStatement(self):\n",
    "\n",
    "        self.file_out.write('<statement>')\n",
    "\n",
    "        if self.tokenizer.current_token == 'let':\n",
    "            self.compileLet()\n",
    "        elif self.tokenizer.current_token == 'if':\n",
    "            self.compileIf()\n",
    "        elif self.tokenizer.current_token == 'while':\n",
    "            self.compileWhile()\n",
    "        elif self.tokenizer.current_token == 'do':\n",
    "            self.compileDo()\n",
    "        elif self.tokenizer.current_token == 'return':\n",
    "            self.compileReturn()\n",
    "                    \n",
    "        self.file_out.write('</statements>')\n",
    "\n",
    "    def compileLet(self):\n",
    "        self.file_out.write('<statementLet>')\n",
    "        \n",
    "        # 'let'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # varName\n",
    "        if self.tokenizer.tokenType == 'identifier':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        # ('[' expression ']')?\n",
    "        if self.tokenizer.current_token == '[':\n",
    "            # '['\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "            \n",
    "            # expression\n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # ']'\n",
    "            if self.tokenizer.current_token == ']':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected ']' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        if self.tokenizer.current_token == '=':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '=' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # ';'\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</statementLet>')\n",
    "    \n",
    "    def compileIf(self):\n",
    "        # 'if' '(' expression ')' '{' statements '}' ('else' '{' statements '}')?\n",
    "        self.file_out.write('<statementIf>')\n",
    "        # 'if' '(' expression ')' '{' statements '}'\n",
    "        # 'if'\n",
    "        self.compile\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '('\n",
    "        if self.tokenizer.symbol() == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.symbol() == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # '{'\n",
    "        if self.tokenizer.symbol() == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.symbol() == '}':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "\n",
    "        # ('else' '{' statements '}')?\n",
    "        if self.tokenizer.current_token == 'else':\n",
    "            #else\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # '{'\n",
    "            if self.tokenizer.symbol() == '{':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # statements\n",
    "            self.compileStatements()\n",
    "            self.advance()\n",
    "\n",
    "            # '}'\n",
    "            if self.tokenizer.symbol() == '}':\n",
    "                self.compile()\n",
    "            else:\n",
    "                raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        #If there is no else statement\n",
    "        else:\n",
    "            self.tokenizer.counter -= 1\n",
    "\n",
    "        self.file_out.write('</statementIf>')\n",
    "    \n",
    "    def compileWhile(self):\n",
    "        # 'while' '(' expression ')' '{' statements '}'\n",
    "        self.file_out.write('<statementWhile>')\n",
    "\n",
    "        # 'while'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '('\n",
    "        if self.tokenizer.symbol() == '(':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '(' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # expression\n",
    "        self.compileExpression()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.symbol() == ')':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # '{'\n",
    "        if self.tokenizer.symbol() == '{':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.symbol() == '}':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</statementWhile>')\n",
    "    \n",
    "    def compileDo(self):\n",
    "        # 'do' subroutineCall ';'\n",
    "        self.file_out.write('<statementDo>')\n",
    "\n",
    "        # 'do'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # subroutineCall\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # ';'\n",
    "        if self.tokenizer.symbol() == ';':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance\n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        \n",
    "        self.file_out.write('</statementDo>')\n",
    "\n",
    "    def compileReturn(self):\n",
    "        self.file_out.write('<statementReturn>')\n",
    "\n",
    "        # 'return'\n",
    "        self.compile()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # expression\n",
    "        if self.tokenizer.current_token != ';':\n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "        \n",
    "        # ';'\n",
    "        if self.tokenizer.symbol() == ';':\n",
    "            self.compile()\n",
    "            self.tokenizer.advance\n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")    \n",
    "\n",
    "        self.file_out.write('</statementReturn>')\n",
    "\n",
    "    def compileExpressionList(self):\n",
    "        #(expression(',' expression)*)?\n",
    "        self.file_out.write('<expressionList>')\n",
    "\n",
    "        #(expression(',' expression)*)?\n",
    "        if self.tokenizer.tokenType in ['int_val', 'string_val', 'indentifier'] or self.tokenizer.current_token in ['true', 'false', 'null', 'this']:\n",
    "            # expression\n",
    "            self.compileExpression()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # (',' expression)*\n",
    "            while self.tokenizer.current_token == ',':\n",
    "                # ','\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "                # expression\n",
    "                self.compileExpression()\n",
    "                self.tokenizer.advance()\n",
    "            \n",
    "            #Atrasando o contador em\n",
    "            self.tokenizer.counter -= 1\n",
    "\n",
    "        self.file_out.write('</expressionList>')\n",
    "\n",
    "    def compileExpression(self):\n",
    "        # term (op term)*\n",
    "        self.file_out.write('<expression>')\n",
    "\n",
    "        # term\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # (op term)*\n",
    "        while self.tokenizer.current_token in self.op:\n",
    "            # op\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # term\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        #Andando com o contador para trás\n",
    "        self.tokenizer.counter -= 1\n",
    "        \n",
    "        self.file_out.write('</expression>')\n",
    "\n",
    "    def compileParameterList(self):\n",
    "        self.file_out.write('<parameterList>')\n",
    "        \n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            # type\n",
    "            self.compile()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            #(',' type varName)*\n",
    "            while (self.tokenizer.current_token == ','):\n",
    "                # ','\n",
    "                self.compile()\n",
    "                self.tokenizer.advance()\n",
    "                \n",
    "                # type\n",
    "                if self.tokenizer.current_token in self.type:\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else: \n",
    "                    raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "                # varName\n",
    "                if self.tokenizer.tokenType() == 'identifier':\n",
    "                    self.compile()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                    raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        self.file_out.write('</parameterList>')\n",
    "\n",
    "    def Close (self):\n",
    "        '''Closes the output file.'''\n",
    "\n",
    "        self.file_out.write('</tokens>')\n",
    "        self.file_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    file_dir = input('Please, insert the file name:')\n",
    "\n",
    "    if file_dir[-1] == '/': #Verifying if the input is a file or a folder\n",
    "        os.chdir(file_dir)\n",
    "        files = glob.glob(\"*.jack\")\n",
    "        dir_name = file_dir.split('/')[-2]\n",
    "        \n",
    "        #code_writer = CodeWriter(dir_name + '.asm')\n",
    "        \n",
    "        # if 'Sys.jack' in files:\n",
    "        #     code_writer.writeCall (arg1='Sys.init', arg2='0')\n",
    "\n",
    "    else:\n",
    "        files = [file_dir]\n",
    "        #code_writer = CodeWriter(files[0] + '.asm')\n",
    "\n",
    "    \n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        arg1 = ''\n",
    "        arg2 = ''\n",
    "\n",
    "        tokenizer = JackTokenizer(file)\n",
    "\n",
    "        while (tokenizer.hasMoreTokens() == True):\n",
    "            tokenizer.advance()\n",
    "            token_type = tokenizer.tokenType()\n",
    "            \n",
    "            if token_type == 'keyword':  \n",
    "                key_word = tokenizer.keyWord()\n",
    "                #code_writer.writePushPop(token_type, arg1, arg2, file)\n",
    "\n",
    "            elif token_type == 'symbol':\n",
    "                symbol = tokenizer.symbol()\n",
    "                #code_writer.writeArithmetic(arg1)\n",
    "\n",
    "            elif token_type == 'identifier':\n",
    "                identifier = tokenizer.identifier()\n",
    "                #code_writer.writeLabel(arg1)\n",
    "\n",
    "            elif token_type == 'int_val':\n",
    "                int_val = tokenizer.intVal()\n",
    "                #code_writer.writeGoto(arg1)\n",
    "\n",
    "            elif token_type == 'string_val':\n",
    "                string_val = tokenizer.stringVal()\n",
    "                #code_writer.writeIf(arg1)\n",
    "\n",
    "    #code_writer.Close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adc1fd1a4b8c764c52852e42b10ea6aaf40f3ebe47b5be7cd49ce1732141673f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
