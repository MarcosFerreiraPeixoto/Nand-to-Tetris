{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re\n",
    "\n",
    "class JackTokenizer:\n",
    "    def __init__(self, file):\n",
    "        with open(file) as f:\n",
    "            '''Opens the input .jack file and gets ready to tokenize it.'''\n",
    "            self.code = f.readlines()\n",
    "            self.code = self.removeWhiteSpace(self.code)\n",
    "            \n",
    "            self.tokens = self.getTokens(self.code)\n",
    "            self.counter = 0\n",
    "            self.current_token = ''         \n",
    "\n",
    "    def hasMoreTokens (self):\n",
    "        '''Are there more commands in the input file?'''\n",
    "        \n",
    "        if self.counter < len(self.tokens):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def advance (self):\n",
    "        '''Reads the next token and makes it the current \n",
    "        token. Should be called only if hasMoreCommands()\n",
    "        is true. Initially there is no current command.'''\n",
    "\n",
    "        \n",
    "        self.current_token = self.tokens[self.counter]\n",
    "        self.counter += 1\n",
    "    \n",
    "    def tokenType (self):\n",
    "        '''Returns the type of the current token, as a\n",
    "        constant.'''\n",
    "        \n",
    "        key_words = ['class', 'constructor', 'function', 'method', 'field', 'static', 'var', 'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this', 'let', 'do', 'if', 'else', 'while', 'return']\n",
    "        symbols = ['{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&', '|', '<', '>', '=', '~']\n",
    "   \n",
    "        if self.current_token in key_words:\n",
    "            return 'keyword'\n",
    "        elif self.current_token in symbols:\n",
    "            return 'symbol'\n",
    "        elif self.current_token.isnumeric():\n",
    "            return 'int_const'\n",
    "        elif self.current_token[0] == '\"':\n",
    "            return 'string_const'\n",
    "        else:\n",
    "            return 'identifier'\n",
    "\n",
    "    def keyWord (self):\n",
    "        return self.current_token.upper()\n",
    "    \n",
    "    def symbol (self):\n",
    "        return self.current_token\n",
    "    \n",
    "    def identifier (self):\n",
    "        return self.current_token\n",
    "    \n",
    "    def intVal (self):\n",
    "        return int(self.current_token)\n",
    "    \n",
    "    def stringVal (self):\n",
    "        return self.current_token.replace('\"','')\n",
    "\n",
    "    @staticmethod\n",
    "    def removeWhiteSpace (code):\n",
    "        \n",
    "        code_without_white_space = []\n",
    "        \n",
    "        for line in code:\n",
    "            line = line.split('\\n', 1)[0]\n",
    "            line = line.split('//', 1)[0]\n",
    "            line = line.split('/**',1)[0]\n",
    "            line = line.split('/*',1)[0]\n",
    "            line = line.strip()\n",
    "            code_without_white_space.append(line)\n",
    "        \n",
    "        code_without_white_space = list(filter(None, code_without_white_space))\n",
    "        \n",
    "        return code_without_white_space\n",
    "\n",
    "    @staticmethod\n",
    "    def getTokens (code_without_white_space):\n",
    "        token_list = []\n",
    "\n",
    "        #Creating a list with all tokens\n",
    "        for code_line in code_without_white_space:\n",
    "            code_line = re.split('(\")', code_line) #Spliting the Strings\n",
    "            j = 0\n",
    "            while (j < len(code_line)):\n",
    "                \n",
    "                #Dealing with StringConstant Tokens\n",
    "                if code_line[j] == '\"':\n",
    "                    token_list.append('\"' + code_line[j+1] + '\"')\n",
    "                    j += 2\n",
    "                \n",
    "                #Dealing with all other tokens\n",
    "                else:\n",
    "                    tokens = re.split('(\\W)', code_line[j])\n",
    "                    for token in tokens:\n",
    "                        token_list.append(token)\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        token_list = [token for token in token_list if (token != '' and token != ' ')]\n",
    "\n",
    "        return token_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeWriter:\n",
    "    def __init__(self, file, tokenizer):\n",
    "        '''Opens the output file/stream and gets ready to write into it.'''\n",
    "        self.file_out = open(file, \"w\")\n",
    "        self.jmps = 0\n",
    "        self.tokenizer = JackTokenizer('test.jack()') ##MODIFICAR PAR self.tokenizer = tokenizer\n",
    "\n",
    "        self.type = ['int', 'char', 'boolean']\n",
    "        self.statements = ['let', 'if', 'while', 'do', 'return']\n",
    "\n",
    "        self.file_out.write('<tokens>')\n",
    "        \n",
    "    def compileClass (self):\n",
    "        '''Writes to the output file the assemply code that implements the\n",
    "        givem arithmetic command.'''\n",
    "\n",
    "        # 'class'\n",
    "        self.file_out.write('<class>')\n",
    "        self.file_out.write('<keyword> class </keyword>')\n",
    "        self.tokenizer.advance()\n",
    "        \n",
    "        # 'className'\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        # '{'\n",
    "        if self.tokenizer.symbol() == '{':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{'\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # 'classVarDec'*\n",
    "        if self.tokenizer.current_token in ['static', 'field']:\n",
    "            self.file_out.write('<classVarDec>')\n",
    "            \n",
    "            while (self.tokenizer.current_token in ['static', 'field']):\n",
    "                self.compileClassVarDec()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "            self.file_out.write('</classVarDec>')\n",
    "        \n",
    "        # 'subroutineDec'*\n",
    "        if self.tokenizer.current_token in ['constructor', 'function', 'method']:\n",
    "            self.file_out.write('<subroutineDec>')\n",
    "            \n",
    "            while (self.tokenizer.current_token in ['constructor', 'function', 'method']):\n",
    "                self.compileSubroutine()\n",
    "                self.tokenizer.advance()\n",
    "\n",
    "            self.file_out.write('</subroutineDec>')\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compileTerm()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        self.file_out.write('</class>')\n",
    "\n",
    "\n",
    "    def compileClassVarDec (self):\n",
    "        \n",
    "        # ('static' | 'field')\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type\n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token != ';'):\n",
    "            \n",
    "            # ','\n",
    "            if self.tokenizer.current_token == ',':\n",
    "                self.compileTerm()              \n",
    "            else:\n",
    "                raise Exception(\"Expected ',' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compileTerm()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "    \n",
    "    def VarDec (self):\n",
    "        \n",
    "        # ('var')\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # type\n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # varName\n",
    "        if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "            \n",
    "\n",
    "        while (self.tokenizer.current_token != ';'):\n",
    "            \n",
    "            # ','\n",
    "            if self.tokenizer.current_token == ',':\n",
    "                self.compileTerm()              \n",
    "            else:\n",
    "                raise Exception(\"Expected ',' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        if self.tokenizer.current_token == ';':\n",
    "                self.compileTerm()              \n",
    "        else:\n",
    "            raise Exception(\"Expected ';' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "    def compileTerm (self):\n",
    "            token_type = self.tokenizer.tokenType()\n",
    "            \n",
    "            if token_type == 'keyword':  \n",
    "                key_word = self.tokenizer.keyWord()\n",
    "                self.file_out.write(f\"<{token_type}> {key_word} </{token_type}>\")\n",
    "\n",
    "            elif token_type == 'symbol':\n",
    "                symbol = self.tokenizer.symbol()\n",
    "                self.file_out.write(f\"<{token_type}> {symbol} </{token_type}>\")\n",
    "               \n",
    "            elif token_type == 'identifier':\n",
    "                identifier = self.tokenizer.identifier()\n",
    "                self.file_out.write(f\"<{token_type}> {identifier} </{token_type}>\")\n",
    "\n",
    "            elif token_type == 'int_val':\n",
    "                int_val = self.tokenizer.intVal()\n",
    "                self.file_out.write(f\"<{token_type}> {int_val} </{token_type}>\")\n",
    "\n",
    "            elif token_type == 'string_val':\n",
    "                string_val = self.tokenizer.stringVal()\n",
    "                self.file_out.write(f\"<{token_type}> {string_val} </{token_type}>\")\n",
    "\n",
    "    def compileSubroutine (self):\n",
    "        #('constructor' | 'function' | 'method')\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        #('void' | 'type')\n",
    "        if self.tokenizer.current_token == 'void':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        elif self.tokenizer.current_token in self.type:\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        \n",
    "        #subroutineName\n",
    "        if self.tokenizer.tokenType == 'identifier':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected identifier\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        #'('\n",
    "        if self.tokenizer.symbol() == '{(}':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected '('\" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # parameterList\n",
    "        self.compileParameterList() #Does not need tokenizer.advance() after.\n",
    "\n",
    "        #')'\n",
    "        if self.tokenizer.symbol() == '{)}':\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "        else:\n",
    "            raise Exception(\"Expected ')' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "        # subroutineBody\n",
    "        if self.tokenizer.current_token == '{':\n",
    "            self.compileSubroutineBody()\n",
    "        else:\n",
    "            raise Exception(\"Expected '{' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "        \n",
    "    def compileSubroutineBody(self):\n",
    "        \n",
    "        # '{'\n",
    "        self.compileTerm()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # varDec*\n",
    "        while self.tokenizer.current_token == 'var':\n",
    "            self.varDec()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "        # statements\n",
    "        self.compileStatements()\n",
    "        self.tokenizer.advance()\n",
    "\n",
    "        # '}'\n",
    "        if self.tokenizer.current_token == '}':\n",
    "            self.compileTerm()\n",
    "        else:\n",
    "            raise Exception(\"Expected '}' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "    def compileStatements(self):\n",
    "        \n",
    "        while(self.tokenizer.current_token in self.statements):\n",
    "            self.compileStatement()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "    def compileStatement(self):\n",
    "        return\n",
    "    \n",
    "    def compileParameterList(self):\n",
    "        self.file_out.write('</parameterList>')\n",
    "        \n",
    "        if self.tokenizer.current_token in self.type:\n",
    "            # type\n",
    "            self.compileTerm()\n",
    "            self.tokenizer.advance()\n",
    "\n",
    "            # varName\n",
    "            if self.tokenizer.tokenType() == 'identifier':\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "            else:\n",
    "                raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "\n",
    "            #(',' type varName)*\n",
    "            while (self.tokenizer.current_token == ','):\n",
    "                # ','\n",
    "                self.compileTerm()\n",
    "                self.tokenizer.advance()\n",
    "                \n",
    "                # type\n",
    "                if self.tokenizer.current_token in self.type:\n",
    "                    self.compileTerm()\n",
    "                    self.tokenizer.advance()\n",
    "                else: \n",
    "                    raise Exception(\"Expected 'void', 'int', 'char' or 'boolean' \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "        \n",
    "                # varName\n",
    "                if self.tokenizer.tokenType() == 'identifier':\n",
    "                    self.compileTerm()\n",
    "                    self.tokenizer.advance()\n",
    "                else:\n",
    "                    raise Exception(\"Expected an identifier \" + f\"instead found: '{self.tokenizer.current_token}'\")\n",
    "    \n",
    "        self.file_out.write('</parameterList>')\n",
    "\n",
    "    def Close (self):\n",
    "        '''Closes the output file.'''\n",
    "\n",
    "        self.file_out.write('</tokens>')\n",
    "        self.file_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    file_dir = input('Please, insert the file name:')\n",
    "\n",
    "    if file_dir[-1] == '/': #Verifying if the input is a file or a folder\n",
    "        os.chdir(file_dir)\n",
    "        files = glob.glob(\"*.jack\")\n",
    "        dir_name = file_dir.split('/')[-2]\n",
    "        \n",
    "        #code_writer = CodeWriter(dir_name + '.asm')\n",
    "        \n",
    "        # if 'Sys.jack' in files:\n",
    "        #     code_writer.writeCall (arg1='Sys.init', arg2='0')\n",
    "\n",
    "    else:\n",
    "        files = [file_dir]\n",
    "        #code_writer = CodeWriter(files[0] + '.asm')\n",
    "\n",
    "    \n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        arg1 = ''\n",
    "        arg2 = ''\n",
    "\n",
    "        tokenizer = JackTokenizer(file)\n",
    "\n",
    "        while (tokenizer.hasMoreTokens() == True):\n",
    "            tokenizer.advance()\n",
    "            token_type = tokenizer.tokenType()\n",
    "            \n",
    "            if token_type == 'keyword':  \n",
    "                key_word = tokenizer.keyWord()\n",
    "                #code_writer.writePushPop(token_type, arg1, arg2, file)\n",
    "\n",
    "            elif token_type == 'symbol':\n",
    "                symbol = tokenizer.symbol()\n",
    "                #code_writer.writeArithmetic(arg1)\n",
    "\n",
    "            elif token_type == 'identifier':\n",
    "                identifier = tokenizer.identifier()\n",
    "                #code_writer.writeLabel(arg1)\n",
    "\n",
    "            elif token_type == 'int_val':\n",
    "                int_val = tokenizer.intVal()\n",
    "                #code_writer.writeGoto(arg1)\n",
    "\n",
    "            elif token_type == 'string_val':\n",
    "                string_val = tokenizer.stringVal()\n",
    "                #code_writer.writeIf(arg1)\n",
    "\n",
    "    #code_writer.Close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adc1fd1a4b8c764c52852e42b10ea6aaf40f3ebe47b5be7cd49ce1732141673f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
